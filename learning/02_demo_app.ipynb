{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# LangChain LCEL Application Implementation\n",
    "\n",
    "**Author**: Sergio Masa Avis  \n",
    "**Date**: October 2025\n",
    "\n",
    "Comprehensive implementation of a LLM application using LangChain and LCEL (LangChain Expression Language). Demonstrates chain composition, prompt templating, and output parsing patterns for AI applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [LangChain and LCEL Overview](#langchain-and-lcel-overview)\n",
    "2. [Architecture and Components](#architecture-and-components)\n",
    "3. [Implementation](#implementation)\n",
    "4. [Trip Planner Application](#trip-planner-application)\n",
    "5. [Advanced Features](#advanced-features)\n",
    "6. [References](#references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "langchain-and-lcel-overview",
   "metadata": {},
   "source": [
    "## 1. LangChain and LCEL Overview\n",
    "\n",
    "**LangChain** is a comprehensive framework for developing applications with large language models (LLMs), providing abstractions for prompt management, chain composition, and integration with external data sources and APIs.\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is a declarative syntax for composing LangChain components into execution chains using the pipe operator (`|`).\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**Chain Composition**: Sequential component execution through standardized interfaces\n",
    "\n",
    "```python\n",
    "chain = prompt | model | parser\n",
    "```\n",
    "\n",
    "**Runnable Interface**: Common protocol for all LangChain components enabling interoperability\n",
    "\n",
    "**Template-Based Prompts**: Dynamic prompt generation with variable substitution\n",
    "\n",
    "### LCEL Benefits\n",
    "\n",
    "| Feature | Benefit | Implementation |\n",
    "|---------|---------|----------------|\n",
    "| **Streaming** | Real-time response generation | Automatic chunked output |\n",
    "| **Async Support** | Concurrent execution | Native async/await compatibility |\n",
    "| **Batch Processing** | Multiple input handling | Vectorized operations |\n",
    "| **Parallelization** | Performance optimization | Automatic dependency resolution |\n",
    "| **Logging** | Observability | Built-in execution tracing |\n",
    "| **Fallbacks** | Error resilience | Alternative execution paths |\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Traditional LLM applications face several challenges:\n",
    "- **Prompt Management**: Hard-coded prompts, difficult versioning\n",
    "- **Component Integration**: Manual wiring between different services\n",
    "- **Error Handling**: Limited resilience and retry mechanisms\n",
    "- **Observability**: Poor visibility into execution flow\n",
    "- **Scalability**: No built-in support for async or batch processing\n",
    "\n",
    "LangChain + LCEL solutions:\n",
    "- Standardized component interfaces through Runnable protocol\n",
    "- Declarative chain composition with automatic optimization\n",
    "- Built-in error handling, retries, and fallback mechanisms\n",
    "- Comprehensive logging and tracing capabilities\n",
    "- Native support for streaming, async, and batch operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-and-components",
   "metadata": {},
   "source": [
    "## 2. Architecture and Components\n",
    "\n",
    "LangChain applications follow a modular architecture with well-defined component responsibilities.\n",
    "\n",
    "### System Architecture\n",
    "\n",
    "![LangChain Architecture](images/02_demo_app/langchain_architecture.png)\n",
    "\n",
    "### Component Deep Dive\n",
    "\n",
    "#### Prompt Templates\n",
    "\n",
    "**Purpose**: Dynamic prompt construction with variable interpolation\n",
    "\n",
    "**Implementation**: `ChatPromptTemplate` for multi-turn conversations\n",
    "\n",
    "**Key Features**:\n",
    "- Variable substitution with `{variable}` syntax\n",
    "- Message role specification (system, user, assistant)\n",
    "- Template validation and type checking\n",
    "\n",
    "#### Language Models\n",
    "\n",
    "**Purpose**: Text generation and completion\n",
    "\n",
    "**Implementation**: `ChatOpenAI` wrapper for OpenAI's API\n",
    "\n",
    "**Configuration Options**:\n",
    "- Model selection (gpt-4, gpt-3.5-turbo, etc.)\n",
    "- Temperature control for randomness\n",
    "- Token limits and streaming options\n",
    "\n",
    "#### Output Parsers\n",
    "\n",
    "**Purpose**: Response formatting and structure extraction\n",
    "\n",
    "**Implementation**: `StrOutputParser` for string extraction\n",
    "\n",
    "**Alternative Parsers**:\n",
    "- `PydanticOutputParser` for structured data\n",
    "- `JSONOutputParser` for JSON responses\n",
    "- `CommaSeparatedListOutputParser` for lists\n",
    "\n",
    "### Data Flow Analysis\n",
    "\n",
    "![LangChain Data Flow](images/02_demo_app/langchain_data_flow.png)\n",
    "\n",
    "The data flow demonstrates the four-step process from user input to final output:\n",
    "\n",
    "1. **Template Variable Injection**: Input parameters are injected into prompt template\n",
    "2. **Prompt Formatting**: System and user messages are properly formatted\n",
    "3. **LLM Processing**: OpenAI API processes the prompt and returns AIMessage\n",
    "4. **Output Parsing**: AIMessage content is extracted as clean string\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31-environment-setup",
   "metadata": {},
   "source": [
    "### 3.1 Environment Setup\n",
    "\n",
    "Import required LangChain components and configure environment variables for API access.\n",
    "\n",
    "**Dependencies**:\n",
    "- `langchain-openai`: OpenAI model integrations\n",
    "- `langchain-core`: Core abstractions and utilities\n",
    "- `python-dotenv`: Environment variable management\n",
    "\n",
    "**Security**: API keys loaded from `.env` file to avoid hardcoding sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Verify API key is available\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Missing OPENAI_API_KEY environment variable\"\n",
    "\n",
    "print(\"LangChain components imported successfully\")\n",
    "print(\"Environment variables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32-model-configuration",
   "metadata": {},
   "source": [
    "### 3.2 Model Configuration\n",
    "\n",
    "Initialize the ChatOpenAI model with specific configuration for consistent, high-quality outputs.\n",
    "\n",
    "**Configuration Parameters**:\n",
    "- `model_name`: GPT-4 for high-quality reasoning and planning\n",
    "- `temperature=0`: Deterministic outputs for reproducible results\n",
    "- Default streaming and async support enabled automatically\n",
    "\n",
    "**Model Selection Rationale**:\n",
    "- GPT-4: Superior reasoning capabilities for complex trip planning\n",
    "- Temperature 0: Ensures consistent recommendations for same inputs\n",
    "- Cost consideration: GPT-4 more expensive but provides better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatOpenAI with GPT-4\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",  # High-quality model for complex reasoning\n",
    "    temperature=0        # Deterministic outputs\n",
    ")\n",
    "\n",
    "print(f\"Model configured: {model.model_name} with temperature={model.temperature}\")\n",
    "print(f\"Streaming enabled: {model.streaming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33-prompt-template",
   "metadata": {},
   "source": [
    "### 3.3 Prompt Template Design\n",
    "\n",
    "Create a structured prompt template using ChatPromptTemplate for multi-turn conversation management.\n",
    "\n",
    "**Template Structure**:\n",
    "- **System Message**: Defines agent role and behavior constraints\n",
    "- **User Message**: Contains the actual query with variable substitution\n",
    "\n",
    "**Variable Substitution**:\n",
    "- `{destination}`: Target location for trip planning\n",
    "- `{preferences}`: User preferences (activities, budget, style)\n",
    "\n",
    "**Design Principles**:\n",
    "- Clear role definition (\"trip planner expert\")\n",
    "- Specific instruction for preference consideration\n",
    "- Flexible variable system for different use cases\n",
    "\n",
    "**Prompt Engineering Best Practices Applied**:\n",
    "- Explicit role assignment for consistent behavior\n",
    "- Context injection through variables\n",
    "- Clear task specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt defining agent behavior\n",
    "system_template = \"\"\"\n",
    "You are a trip planner expert. Help me plan a trip to {destination}.\n",
    "Consider my preferences for {preferences}.\n",
    "Provide specific, actionable recommendations with practical details.\n",
    "\"\"\"\n",
    "\n",
    "# Create ChatPromptTemplate with system and user messages\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_template),\n",
    "    ('user', 'What should I do in {destination}? Please provide detailed recommendations.')\n",
    "])\n",
    "\n",
    "print(f\"Prompt template created with {len(prompt_template.messages)} messages:\")\n",
    "print(\"1. System message with role definition\")\n",
    "print(\"2. User message with query template\")\n",
    "print(f\"\\nVariables used: {prompt_template.input_variables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34-output-parser",
   "metadata": {},
   "source": [
    "### 3.4 Output Parser Configuration\n",
    "\n",
    "Configure output parsing to extract clean string responses from model outputs.\n",
    "\n",
    "**StrOutputParser Features**:\n",
    "- Extracts `.content` field from AIMessage objects\n",
    "- Returns clean string without metadata\n",
    "- Compatible with all text-based LLM outputs\n",
    "- No additional configuration required\n",
    "\n",
    "**Alternative Parsers for Different Use Cases**:\n",
    "\n",
    "| Parser Type | Use Case | Output Format |\n",
    "|-------------|----------|---------------|\n",
    "| `StrOutputParser` | Simple text responses | `str` |\n",
    "| `PydanticOutputParser` | Structured data | Custom Pydantic models |\n",
    "| `JSONOutputParser` | JSON responses | `dict` |\n",
    "| `CommaSeparatedListOutputParser` | List outputs | `list[str]` |\n",
    "| `DatetimeOutputParser` | Date/time parsing | `datetime` |\n",
    "\n",
    "**Implementation Note**: StrOutputParser selected for simplicity and compatibility with trip planning text outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "output-parser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize string output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "print(f\"Output parser configured: {parser.__class__.__name__}\")\n",
    "print(\"Purpose: Extract string content from AIMessage objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35-chain-composition",
   "metadata": {},
   "source": [
    "### 3.5 LCEL Chain Composition\n",
    "\n",
    "Compose the complete processing chain using LCEL pipe operator syntax.\n",
    "\n",
    "**Chain Definition**:\n",
    "```python\n",
    "chain = prompt_template | model | parser\n",
    "```\n",
    "\n",
    "**Execution Flow**:\n",
    "1. **Input** → `prompt_template`: Variable substitution and message formatting\n",
    "2. **Formatted Prompt** → `model`: LLM processing and response generation\n",
    "3. **AIMessage** → `parser`: Content extraction and string conversion\n",
    "4. **Output** → Clean string response\n",
    "\n",
    "**LCEL Benefits in This Chain**:\n",
    "- **Automatic Type Checking**: Input/output compatibility validation\n",
    "- **Streaming Support**: Real-time response generation capability\n",
    "- **Error Propagation**: Automatic error handling across components\n",
    "- **Observability**: Built-in logging and tracing\n",
    "\n",
    "**Chain Properties**:\n",
    "- **Stateless**: No internal state between invocations\n",
    "- **Reusable**: Can be called multiple times with different inputs\n",
    "- **Composable**: Can be used as component in larger chains\n",
    "- **Serializable**: Can be saved and loaded for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chain-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose the complete chain using LCEL\n",
    "trip_planner_chain = prompt_template | model | parser\n",
    "\n",
    "print(\"Trip planner chain created successfully\")\n",
    "print(\"Chain components: 3\")\n",
    "print(\"  1. ChatPromptTemplate (prompt formatting)\")\n",
    "print(\"  2. ChatOpenAI (LLM processing)\")\n",
    "print(\"  3. StrOutputParser (output extraction)\")\n",
    "print()\n",
    "print(\"Chain capabilities:\")\n",
    "print(\"  ✓ Synchronous execution\")\n",
    "print(\"  ✓ Asynchronous execution\")\n",
    "print(\"  ✓ Streaming support\")\n",
    "print(\"  ✓ Batch processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trip-planner-application",
   "metadata": {},
   "source": [
    "## 4. Trip Planner Application\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41-function-interface",
   "metadata": {},
   "source": [
    "### 4.1 Function Interface Design\n",
    "\n",
    "Create a clean, reusable function interface for the trip planning chain.\n",
    "\n",
    "**Design Principles**:\n",
    "- **Type Hints**: Clear input/output type specification\n",
    "- **Documentation**: Comprehensive docstring with parameters and return values\n",
    "- **Error Handling**: Graceful handling of API failures\n",
    "- **Input Validation**: Parameter validation before chain execution\n",
    "\n",
    "**Function Signature**:\n",
    "```python\n",
    "def plan_trip(destination: str, preferences: str) -> str\n",
    "```\n",
    "\n",
    "**Implementation Strategy**:\n",
    "- Dictionary-based input for chain compatibility\n",
    "- Synchronous execution for simplicity\n",
    "- String return type for easy integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "function-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_trip(destination: str, preferences: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate trip recommendations for a destination based on user preferences.\n",
    "\n",
    "    Args:\n",
    "        destination: Target location for the trip (e.g., \"Paris\", \"Tokyo\")\n",
    "        preferences: User preferences and requirements (e.g., \"museums, food, budget-friendly\")\n",
    "\n",
    "    Returns:\n",
    "        str: Detailed trip recommendations and suggestions\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not destination or not destination.strip():\n",
    "        raise ValueError(\"Destination cannot be empty\")\n",
    "    if not preferences or not preferences.strip():\n",
    "        raise ValueError(\"Preferences cannot be empty\")\n",
    "\n",
    "    try:\n",
    "        # Prepare input data for chain\n",
    "        input_data = {\n",
    "            \"destination\": destination.strip(),\n",
    "            \"preferences\": preferences.strip()\n",
    "        }\n",
    "\n",
    "        # Execute the chain\n",
    "        result = trip_planner_chain.invoke(input_data)\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log error and re-raise with context\n",
    "        error_msg = f\"Trip planning failed for {destination}: {str(e)}\"\n",
    "        raise Exception(error_msg) from e\n",
    "\n",
    "def render_markdown(content: str):\n",
    "    \"\"\"\n",
    "    Render the given content as Markdown in the notebook.\n",
    "\n",
    "    Args:\n",
    "        content: The string content to render as Markdown\n",
    "    \"\"\"\n",
    "    display(Markdown(content))\n",
    "\n",
    "print(\"Trip planning function defined\")\n",
    "print(\"Function signature: plan_trip(destination: str, preferences: str) -> str\")\n",
    "print(\"Helper function: render_markdown() for enhanced output display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42-practical-examples",
   "metadata": {},
   "source": [
    "### 4.2 Practical Examples\n",
    "\n",
    "Demonstrate the trip planner with various real-world scenarios.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421-paris-cultural-trip",
   "metadata": {},
   "source": [
    "#### 4.2.1 Paris Cultural Trip\n",
    "\n",
    "Plan a cultural trip to Paris focusing on art and cuisine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Cultural trip to Paris\n",
    "destination_1 = \"Paris\"\n",
    "preferences_1 = \"art museums, French cuisine, cultural experiences\"\n",
    "\n",
    "print(f\"TRIP PLAN: {destination_1} (Art Museums & French Cuisine)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result_1 = plan_trip(destination_1, preferences_1)\n",
    "render_markdown(result_1)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422-tokyo-tech-adventure",
   "metadata": {},
   "source": [
    "#### 4.2.2 Tokyo Technology and Food Adventure\n",
    "\n",
    "Explore Tokyo's technology scene and culinary diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Technology and food adventure in Tokyo\n",
    "destination_2 = \"Tokyo\"\n",
    "preferences_2 = \"technology, innovation, authentic Japanese food, modern experiences\"\n",
    "\n",
    "print(f\"TRIP PLAN: {destination_2} (Technology & Food)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result_2 = plan_trip(destination_2, preferences_2)\n",
    "render_markdown(result_2)\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423-budget-backpacking",
   "metadata": {},
   "source": [
    "#### 4.2.3 Budget Backpacking in Southeast Asia\n",
    "\n",
    "Plan an economical adventure through Thailand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Budget backpacking adventure\n",
    "destination_3 = \"Thailand\"\n",
    "preferences_3 = \"budget backpacking, adventure activities, cultural experiences, low cost\"\n",
    "\n",
    "print(f\"TRIP PLAN: {destination_3} (Budget Backpacking)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result_3 = plan_trip(destination_3, preferences_3)\n",
    "render_markdown(result_3)  # Fixed: was result_1, now correct result_3\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43-input-validation",
   "metadata": {},
   "source": [
    "### 4.3 Input Validation and Error Handling\n",
    "\n",
    "Demonstrate the function's error handling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ERROR HANDLING DEMONSTRATIONS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: Empty destination\n",
    "try:\n",
    "    plan_trip(\"\", \"cultural experiences\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\nTest 1: Empty destination\")\n",
    "    print(f\"✓ Caught expected error: {e}\")\n",
    "\n",
    "# Test 2: Empty preferences\n",
    "try:\n",
    "    plan_trip(\"Paris\", \"\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\nTest 2: Empty preferences\")\n",
    "    print(f\"✓ Caught expected error: {e}\")\n",
    "\n",
    "# Test 3: Whitespace only\n",
    "try:\n",
    "    plan_trip(\"   \", \"museums\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\nTest 3: Whitespace-only inputs\")\n",
    "    print(f\"✓ Caught expected error: {e}\")\n",
    "\n",
    "print(\"\\nAll error handling tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-features",
   "metadata": {},
   "source": [
    "## 5. Advanced Features\n",
    "\n",
    "Explore advanced LCEL capabilities for enhanced functionality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51-streaming-support",
   "metadata": {},
   "source": [
    "### 5.1 Streaming Support\n",
    "\n",
    "Implement real-time response streaming for better user experience.\n",
    "\n",
    "**Streaming Benefits**:\n",
    "- **Reduced perceived latency**: Users see responses immediately\n",
    "- **Better UX**: Progressive content loading\n",
    "- **Chunked processing**: Handle long responses efficiently\n",
    "- **Early termination**: Stop generation if needed\n",
    "\n",
    "**LCEL Streaming**: Automatic support through chain composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_trip_streaming(destination: str, preferences: str):\n",
    "    \"\"\"\n",
    "    Stream trip recommendations in real-time for better user experience.\n",
    "\n",
    "    Args:\n",
    "        destination: Target location\n",
    "        preferences: User preferences\n",
    "\n",
    "    Yields:\n",
    "        str: Streaming chunks of the response\n",
    "    \"\"\"\n",
    "    input_data = {\"destination\": destination, \"preferences\": preferences}\n",
    "\n",
    "    # Stream the response\n",
    "    for chunk in trip_planner_chain.stream(input_data):\n",
    "        yield chunk\n",
    "\n",
    "# Demonstrate streaming\n",
    "print(\"STREAMING DEMO: Rome trip planning\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "\n",
    "# Stream Rome trip planning response\n",
    "for chunk in plan_trip_streaming(\"Rome\", \"ancient history, Italian cuisine\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nSTREAMING COMPLETE\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52-batch-processing",
   "metadata": {},
   "source": [
    "### 5.2 Batch Processing\n",
    "\n",
    "Process multiple trip requests simultaneously for efficiency.\n",
    "\n",
    "**Batch Benefits**:\n",
    "- **Performance**: Parallel API calls\n",
    "- **Cost efficiency**: Reduced overhead\n",
    "- **Throughput**: Handle multiple requests\n",
    "- **Resource optimization**: Better API utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_trips_batch(requests: list[dict]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Process multiple trip planning requests in batch for efficiency.\n",
    "\n",
    "    Args:\n",
    "        requests: List of {\"destination\": str, \"preferences\": str}\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Trip plans corresponding to each request\n",
    "    \"\"\"\n",
    "    return trip_planner_chain.batch(requests)\n",
    "\n",
    "# Demonstrate batch processing\n",
    "print(\"BATCH PROCESSING DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare batch requests\n",
    "batch_requests = [\n",
    "    {\n",
    "        \"destination\": \"London\",\n",
    "        \"preferences\": \"museums, theater, British culture\"\n",
    "    },\n",
    "    {\n",
    "        \"destination\": \"Bali\",\n",
    "        \"preferences\": \"wellness, nature, spiritual experiences\"\n",
    "    },\n",
    "    {\n",
    "        \"destination\": \"New York\",\n",
    "        \"preferences\": \"art galleries, diverse food, urban exploration\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nProcessing {len(batch_requests)} trip requests simultaneously...\\n\")\n",
    "\n",
    "# Execute batch processing\n",
    "batch_results = plan_trips_batch(batch_requests)\n",
    "\n",
    "print(\"BATCH RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display batch results summary\n",
    "for i, (request, result) in enumerate(zip(batch_requests, batch_results), 1):\n",
    "    destination = request[\"destination\"]\n",
    "    preferences = request[\"preferences\"].split(\",\")[0].title() + \" & \" + request[\"preferences\"].split(\",\")[1].title()\n",
    "\n",
    "    print(f\"\\n{i}. {destination.upper()} ({preferences})\")\n",
    "    print(f\"   Result length: {len(result):,} characters\")\n",
    "    print(f\"   ✓ Plan generated successfully\")\n",
    "\n",
    "print(f\"\\nAll {len(batch_requests)} requests completed successfully!\")\n",
    "print(\"Total processing time optimized through batch execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53-async-processing",
   "metadata": {},
   "source": [
    "### 5.3 Asynchronous Processing\n",
    "\n",
    "Implement async support for non-blocking operations.\n",
    "\n",
    "**Async Benefits**:\n",
    "- **Concurrency**: Handle multiple requests simultaneously\n",
    "- **Scalability**: Better resource utilization\n",
    "- **Responsiveness**: Non-blocking I/O operations\n",
    "- **Integration**: Compatible with async web frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "async-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def plan_trip_async(destination: str, preferences: str) -> str:\n",
    "    \"\"\"\n",
    "    Asynchronously plan a trip for non-blocking execution.\n",
    "\n",
    "    Args:\n",
    "        destination: Target location\n",
    "        preferences: User preferences\n",
    "\n",
    "    Returns:\n",
    "        str: Trip recommendations\n",
    "    \"\"\"\n",
    "    input_data = {\"destination\": destination, \"preferences\": preferences}\n",
    "    result = await trip_planner_chain.ainvoke(input_data)\n",
    "    return result\n",
    "\n",
    "async def async_demo():\n",
    "    \"\"\"Demonstrate async trip planning.\"\"\"\n",
    "    print(\"ASYNC PROCESSING DEMO\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nStarting async trip planning...\")\n",
    "\n",
    "    # Execute async trip planning\n",
    "    result = await plan_trip_async(\"Berlin\", \"history, nightlife, alternative culture\")\n",
    "\n",
    "    print(f\"✓ Berlin trip plan completed ({len(result):,} characters)\")\n",
    "    print(\"\\nASYNC PROCESSING SUCCESSFUL\")\n",
    "    print(\"Async execution enables non-blocking operations and better scalability.\")\n",
    "\n",
    "# Run async demo\n",
    "await async_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "references",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "\n",
    "**Official Documentation**:\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [LCEL (LangChain Expression Language)](https://python.langchain.com/docs/concepts/lcel)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)\n",
    "- [ChatOpenAI Integration](https://python.langchain.com/docs/integrations/chat/openai)\n",
    "\n",
    "**Core Concepts**:\n",
    "- [Prompt Templates](https://python.langchain.com/docs/concepts/prompt_templates)\n",
    "- [Output Parsers](https://python.langchain.com/docs/concepts/output_parsers)\n",
    "- [Runnable Interface](https://python.langchain.com/docs/concepts/runnables)\n",
    "- [Streaming in LangChain](https://python.langchain.com/docs/concepts/streaming)\n",
    "\n",
    "**Advanced Features**:\n",
    "- [LangChain Agents](https://python.langchain.com/docs/concepts/agents)\n",
    "- [Memory Systems](https://python.langchain.com/docs/concepts/memory)\n",
    "- [Vector Stores](https://python.langchain.com/docs/concepts/vectorstores)\n",
    "- [Callbacks and Monitoring](https://python.langchain.com/docs/concepts/callbacks)\n",
    "\n",
    "**Production Deployment**:\n",
    "- [LangServe](https://python.langchain.com/docs/langserve) - REST API deployment\n",
    "- [LangSmith](https://smith.langchain.com/) - Monitoring and evaluation\n",
    "- [OpenAI Best Practices](https://platform.openai.com/docs/guides/production-best-practices)\n",
    "\n",
    "**Related Papers**:\n",
    "- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)\n",
    "- [Language Models as Tool Users](https://arxiv.org/abs/2302.04761)\n",
    "- [ReAct: Synergizing Reasoning and Acting](https://arxiv.org/abs/2210.03629)\n",
    "\n",
    "**Community Resources**:\n",
    "- [LangChain GitHub Repository](https://github.com/langchain-ai/langchain)\n",
    "- [LangChain Community](https://github.com/langchain-ai/langchain-community)\n",
    "- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "\n",
    "---\n",
    "\n",
    "*Last updated: October 2025*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
